# LORA : Low-Rank Adaptation of Large Language Models
LoRA et le finetuning

- [ ] back propagation
SI je comprends bien, il est possible de switch les LoRA sur un model. Ainsi, plus facile de switch et de faire plusieurs essais ou tests.

- [ ] Possible de faire un LoRA par dessus un LoRA ?
- [ ] Comment trouver le meilleur rank ?
